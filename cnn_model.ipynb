{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REV Object detect Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=P size=640x360>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "from label_manager import LabelManager\n",
    "import torch\n",
    "\n",
    "\n",
    "obj_entity_list = []\n",
    "obj_entity_list.append({'Entity': 'Background', 'ID': 0, 'Colour': [0,0,0]})\n",
    "\n",
    "with open(\"object.yaml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    for ele in config['DRIVING_objects']:\n",
    "        entity_map = {}\n",
    "        entity_map['Entity'] = ele['Entity']\n",
    "        entity_map['ID'] = ele['ID']\n",
    "        entity_map['Colour'] = ele['Colour']\n",
    "        obj_entity_list.append(entity_map)\n",
    "\n",
    "colour_list = []\n",
    "\n",
    "for entity in obj_entity_list:\n",
    "    cur_colour = entity['Colour']\n",
    "    colour_list += cur_colour\n",
    "\n",
    "Image.open('input/4.jpg')\n",
    "\n",
    "mask = Image.open('mask/0.png')\n",
    "# mask = np.array(mask)\n",
    "\n",
    "# obj_ids = np.unique(mask)\n",
    "# # first id is the background, so remove it\n",
    "# obj_ids = obj_ids[1:]\n",
    "# print(obj_ids)\n",
    "\n",
    "# print('mask shape', mask.shape)\n",
    "\n",
    "# label_manager = LabelManager()\n",
    "# label_num = label_manager.get_num()\n",
    "# mask_label = np.zeros((label_num, mask.shape[0], mask.shape[1]), np.uint8)\n",
    "# print('mask_label', mask_label.shape)\n",
    "\n",
    "# # instances are encoded as different colors\n",
    "# obj_ids = np.unique(mask)\n",
    "# # first id is the background, so remove it\n",
    "# obj_ids = obj_ids[1:]\n",
    "# print('obj_ids', obj_ids)\n",
    "\n",
    "# print('obj_ids [1][None][None]', obj_ids[1, None, None])\n",
    "\n",
    "# for i, id in enumerate(obj_ids):\n",
    "#     mask_label[id] = mask == obj_ids[i, None, None]\n",
    "#     print('obj_ids(i)', obj_ids[i, None, None])\n",
    "\n",
    "# masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "# print('masks: ', masks.shape)\n",
    "# labels = torch.ones((1,), dtype=torch.int64)\n",
    "# print('labels', labels)\n",
    "# labels = torch.as_tensor(obj_ids, dtype=torch.int64)\n",
    "# print('labels', labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mask.putpalette(\n",
    "    [\n",
    "        0, 0, 0,\n",
    "        66, 135, 245,\n",
    "        10, 46, 110,\n",
    "        63, 89, 132,\n",
    "        174, 146, 132,\n",
    "        243, 221, 18,\n",
    "        1, 7, 53,\n",
    "        215, 245, 22\n",
    "    ]\n",
    ")\n",
    "mask\n",
    "\n",
    "# reference_mask = Image.open('FudanPed00001_mask.png')\n",
    "# reference_mask.getpixel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1, 2: 1, 3: 1, 4: 1, 5: 10, 6: 10, 7: 4}\n",
      "[1 2 3 4 5 6 7]\n",
      "tensor([ 1,  1,  1,  1, 10, 10,  4])\n",
      "(7, 360, 640)\n",
      "torch.Size([7, 4])\n",
      "tensor([ 1,  1,  1,  1, 10, 10,  4])\n",
      "torch.Size([7, 360, 640])\n",
      "tensor([0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0])\n",
      "torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open('mask/0.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "print(config)\n",
    "label_list = list(config.values())\n",
    "\n",
    "mask = np.array(mask)\n",
    "\n",
    "obj_ids = np.unique(mask)\n",
    "# first id is the background, so remove it\n",
    "obj_ids = obj_ids[1:]\n",
    "labels = torch.as_tensor(label_list, dtype=torch.int64)\n",
    "print(obj_ids)\n",
    "print(labels)\n",
    "\n",
    "masks = mask == obj_ids[:, None, None]\n",
    "print(masks.shape)\n",
    "num_objs = len(obj_ids)\n",
    "boxes = []\n",
    "for i in range(num_objs):\n",
    "    pos = np.where(masks[i])\n",
    "    xmin = np.min(pos[1])\n",
    "    xmax = np.max(pos[1])\n",
    "    ymin = np.min(pos[0])\n",
    "    ymax = np.max(pos[0])\n",
    "    boxes.append([xmin, ymin, xmax, ymax])\n",
    "boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "print(boxes.shape)\n",
    "labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "print(labels)\n",
    "print(masks.shape)\n",
    "\n",
    "image_id = torch.tensor([0])\n",
    "area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "# suppose all instances are not crowd\n",
    "iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "print(image_id)\n",
    "print(iscrowd)\n",
    "print(area.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "class RevDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.imgs = list(sorted(os.listdir('input/')))\n",
    "        self.masks = list(sorted(os.listdir('mask/')))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join('input/', self.imgs[index])\n",
    "        mask_path = os.path.join('mask/', self.masks[index])\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        mask = Image.open(mask_path)\n",
    "        \n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        \n",
    "        # split the color-encoded mask into a set of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "        \n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([index])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        # print(target)\n",
    "\n",
    "\n",
    "        return img, target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def build_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn()\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Stop here if you are fine-tunning Faster-RCNN\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "\n",
    "def main():\n",
    "    # train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # our dataset has 3 classes only - background, vehicle road lane\n",
    "    num_classes = 3\n",
    "    # use our dataset and defined transformations\n",
    "    dataset = RevDataset('PennFudanPed', get_transform(train=True))\n",
    "    dataset_test = RevDataset('PennFudanPed', get_transform(train=False))\n",
    "\n",
    "    # split the dataset in train and test set\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "    # define training and validation data loaders\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    # get the model using our helper function\n",
    "    model = build_model(num_classes)\n",
    "    print('build mode done')\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "    # and a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "\n",
    "    # let's train it for 5 epochs\n",
    "    num_epochs = 5\n",
    "    print('Lets train the model')\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "        evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "    print(\"That's it!\")\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'mask-rcnn-pedestrian.pt')\n",
    "\n",
    "# set to evaluation mode\n",
    "model.eval()\n",
    "CLASS_NAMES = ['__background__', 'car']\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_coloured_mask(mask):\n",
    "    \"\"\"\n",
    "    random_colour_masks\n",
    "      parameters:\n",
    "        - image - predicted masks\n",
    "      method:\n",
    "        - the masks of each predicted object is given random colour for visualization\n",
    "    \"\"\"\n",
    "    colours = [[0, 255, 0],[0, 0, 255],[255, 0, 0],[0, 255, 255],[255, 255, 0],[255, 0, 255],[80, 70, 180],[250, 80, 190],[245, 145, 50],[70, 150, 250],[50, 190, 190]]\n",
    "    r = np.zeros_like(mask).astype(np.uint8)\n",
    "    g = np.zeros_like(mask).astype(np.uint8)\n",
    "    b = np.zeros_like(mask).astype(np.uint8)\n",
    "    r[mask == 1], g[mask == 1], b[mask == 1] = colours[random.randrange(0,10)]\n",
    "    coloured_mask = np.stack([r, g, b], axis=2)\n",
    "    return coloured_mask\n",
    "\n",
    "def get_prediction(img_path, confidence):\n",
    "    \"\"\"\n",
    "    get_prediction\n",
    "      parameters:\n",
    "        - img_path - path of the input image\n",
    "        - confidence - threshold to keep the prediction or not\n",
    "      method:\n",
    "        - Image is obtained from the image path\n",
    "        - the image is converted to image tensor using PyTorch's Transforms\n",
    "        - image is passed through the model to get the predictions\n",
    "        - masks, classes and bounding boxes are obtained from the model and soft masks are made binary(0 or 1) on masks\n",
    "          ie: eg. segment of cat is made 1 and rest of the image is made 0\n",
    "    \n",
    "    \"\"\"\n",
    "    img = Image.open(img_path)\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    img = transform(img)\n",
    "\n",
    "    img = img.to(device)\n",
    "    pred = model([img])\n",
    "    \n",
    "    # print('hg84---', pred[0].keys())\n",
    "    for cur_p in pred:\n",
    "        print('hg84', cur_p['scores'])\n",
    "\n",
    "    pred_score = list(pred[0]['scores'].detach().cpu().numpy())\n",
    "    pred_t = [pred_score.index(x) for x in pred_score if x>confidence][-1]\n",
    "    masks = (pred[0]['masks']>0.5).squeeze().detach().cpu().numpy()\n",
    "    # print(pred[0]['labels'].numpy().max())\n",
    "    pred_class = [CLASS_NAMES[i] for i in list(pred[0]['labels'].cpu().numpy())]\n",
    "    pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().cpu().numpy())]\n",
    "    masks = masks[:pred_t+1]\n",
    "    pred_boxes = pred_boxes[:pred_t+1]\n",
    "    pred_class = pred_class[:pred_t+1]\n",
    "    return masks, pred_boxes, pred_class\n",
    "    # return pred_boxes, pred_class\n",
    "\n",
    "def segment_instance(img_path, confidence=0.5, rect_th=2, text_size=2, text_th=2):\n",
    "    \"\"\"\n",
    "    segment_instance\n",
    "      parameters:\n",
    "        - img_path - path to input image\n",
    "        - confidence- confidence to keep the prediction or not\n",
    "        - rect_th - rect thickness\n",
    "        - text_size\n",
    "        - text_th - text thickness\n",
    "      method:\n",
    "        - prediction is obtained by get_prediction\n",
    "        - each mask is given random color\n",
    "        - each mask is added to the image in the ration 1:0.8 with opencv\n",
    "        - final output is displayed\n",
    "    \"\"\"\n",
    "    masks, boxes, pred_cls = get_prediction(img_path, confidence)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    for i in range(len(boxes)):\n",
    "      rgb_mask = get_coloured_mask(masks[i])\n",
    "      img = cv2.addWeighted(img, 1, rgb_mask, 0.5, 0)\n",
    "      print(boxes[i])\n",
    "      pt1 = tuple([int(j) for j in boxes[i][0]])\n",
    "      pt2 = tuple([int(j) for j in boxes[i][1]])\n",
    "      cv2.rectangle(img, boxes[i][0], boxes[i][1],color=(0, 255, 0), thickness=rect_th)\n",
    "      cv2.putText(img,pred_cls[i], boxes[i][0], cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n",
    "    plt.figure(figsize=(20,30))\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "segment_instance('input/50.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shawn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
