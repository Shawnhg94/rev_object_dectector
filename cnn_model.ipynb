{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REV Object detect Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=P size=640x360>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "\n",
    "obj_entity_list = []\n",
    "obj_entity_list.append({'Entity': 'Background', 'ID': 0, 'Colour': [0,0,0]})\n",
    "\n",
    "with open(\"object.yaml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    for ele in config['DRIVING_objects']:\n",
    "        entity_map = {}\n",
    "        entity_map['Entity'] = ele['Entity']\n",
    "        entity_map['ID'] = ele['ID']\n",
    "        entity_map['Colour'] = ele['Colour']\n",
    "        obj_entity_list.append(entity_map)\n",
    "\n",
    "colour_list = []\n",
    "\n",
    "for entity in obj_entity_list:\n",
    "    cur_colour = entity['Colour']\n",
    "    colour_list += cur_colour\n",
    "\n",
    "#print(colour_list)\n",
    "\n",
    "Image.open('input/0.jpg')\n",
    "\n",
    "mask = Image.open('mask/0.png')\n",
    "mask.putpalette(colour_list)\n",
    "mask\n",
    "\n",
    "# reference_mask = Image.open('FudanPed00001_mask.png')\n",
    "# reference_mask.getpixel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "class RevDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.imgs = list(sorted(os.listdir('input/')))\n",
    "        self.masks = list(sorted(os.listdir('mask/')))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join('input/', self.imgs[index])\n",
    "        mask_path = os.path.join('mask/', self.masks[index])\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        mask = Image.open(mask_path)\n",
    "        \n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        \n",
    "        # split the color-encoded mask into a set of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "        \n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([index])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "\n",
    "        return img, target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def build_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Stop here if you are fine-tunning Faster-RCNN\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shawn/miniconda3/envs/cits5508/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/shawn/miniconda3/envs/cits5508/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shawn/miniconda3/envs/cits5508/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build mode done\n",
      "Lets train the model\n",
      "Epoch: [0]  [ 0/26]  eta: 4:23:25  lr: 0.000205  loss: 5.0829 (5.0829)  loss_classifier: 0.8137 (0.8137)  loss_box_reg: 0.0891 (0.0891)  loss_mask: 4.0867 (4.0867)  loss_objectness: 0.0785 (0.0785)  loss_rpn_box_reg: 0.0149 (0.0149)  time: 607.8867  data: 0.2726  max mem: 3525\n"
     ]
    }
   ],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "\n",
    "def main():\n",
    "    # train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # our dataset has 3 classes only - background, vehicle road lane\n",
    "    num_classes = 3\n",
    "    # use our dataset and defined transformations\n",
    "    dataset = RevDataset('PennFudanPed', get_transform(train=True))\n",
    "    dataset_test = RevDataset('PennFudanPed', get_transform(train=False))\n",
    "\n",
    "    # split the dataset in train and test set\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "    # define training and validation data loaders\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    # get the model using our helper function\n",
    "    model = build_model(num_classes)\n",
    "    print('build mode done')\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "    # and a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "\n",
    "    # let's train it for 5 epochs\n",
    "    num_epochs = 5\n",
    "    print('Lets train the model')\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "        evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "    print(\"That's it!\")\n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cits5508",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
